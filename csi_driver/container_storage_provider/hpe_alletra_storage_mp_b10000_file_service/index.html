<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Hewlett Packard Enterprise" /><link rel="canonical" href="https://scod.hpedev.io/csi_driver/container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.html" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <meta property="og:title" content="HPE Storage Container Orchestrator Documentation (SCOD)"/>
    <meta property="og:description" content="This is an umbrella documentation project for all container integrations surrounding HPE block, file and object storage. Tailored for IT ops, developers and technology partners."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://scod.hpedev.io"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://scod.hpedev.io/img/hpe-social-og-image01.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
    
    <title>HPE Alletra Storage MP B10000 File Service - SCOD.HPEDEV.IO</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../css/hpedev.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "HPE Alletra Storage MP B10000 File Service";
        var mkdocs_page_input_path = "csi_driver/container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.md";
        var mkdocs_page_url = "/csi_driver/container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.html";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-PC28RTKKTW"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', "G-PC28RTKKTW");
      </script>
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../..">
          <img src="../../../img/hpe.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE CSI DRIVER FOR KUBERNETES</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../using.html">Using</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Container Storage Providers</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../hpe_alletra_storage_mp_b10000/index.html">HPE Alletra Storage MP B10000, Alletra 9000, Primera and 3PAR</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="#">HPE Alletra Storage MP B10000 File Service</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#platform_requirements">Platform Requirements</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#storageclass_parameters">StorageClass parameters</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_subpath_inline_nfs_and_static_nfs_persistent_volumes">Using subPath, Inline NFS and Static NFS Persistent Volumes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#subpath">subPath</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#inline_nfs">Inline NFS</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#static_nfs_persistent_volumes">Static NFS Persistent Volumes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#static_provisioning_for_orphaned_persistent_volumes">Static Provisioning for Orphaned Persistent Volumes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#support">Support</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../hpe_alletra_6000/index.html">HPE Alletra 5000/6000 and Nimble Storage</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../metrics.html">Metrics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../monitor.html">Pod Monitor</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations.html">Auxiliary Operations</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../diagnostics.html">Diagnostics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Partner Ecosystems</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/hpe_morpheus/install.html">HPE Morpheus Kubernetes Service</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/hpe_ezmeral/install.html">HPE Ezmeral Runtime Enterprise</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/amazon_eks_anywhere/index.html">Amazon EKS Anywhere</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/canonical/index.html">Canonical</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/cohesity/index.html">Cohesity</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/commvault/index.html">Commvault</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/kasten/index.html">Veeam Kasten</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/mirantis/index.html">Mirantis</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/redhat_openshift/index.html">Red Hat OpenShift</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/suse_virtualization/index.html">SUSE Virtualization</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/suse_rancher/index.html">SUSE Rancher</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/tkgi/index.html">Tanzu TKGI</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../partners/vmware/index.html">VMware</a>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE COSI DRIVER FOR KUBERNETES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cosi_driver/index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cosi_driver/deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cosi_driver/using.html">Using</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cosi_driver/diagnostics.html">Diagnostics</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE GREENLAKE FOR FILE STORAGE</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../filex_csi_driver/index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../filex_csi_driver/deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../filex_csi_driver/using.html">Using</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEARN</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../learn/persistent_storage/index.html">Persistent Storage for Kubernetes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../learn/introduction_to_containers/index.html">For HPE partners:<br />&nbsp;&nbsp; Introduction to Containers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../learn/csi_primitives/index.html">Introduction to CSI Primitives</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../learn/video_gallery/index.html">Video Gallery</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../learn/csi_workshop/index.html">Interactive CSI Workshop</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">COMMUNITY</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://developer.hpe.com">HPE Developer</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://slack.hpedev.io">Sign up to HPE Developer on Slack</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage/scod/issues/new?title=I have some feedback on SCOD">Got feedback?</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EXTERNAL LINKS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage/scod">SCOD on GitHub</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage">HPE Storage on GitHub</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://hpe.com/storage/containers">Storage for Containers on hpe.com</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../legal/support/index.html">Support</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../legal/license/index.html">License</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../legal/notices/index.html">Notices</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGACY DRIVERS AND PLUGINS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../legacy/index.html">Docker, FlexVolume and CSPs</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">SCOD.HPEDEV.IO</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">HPE CSI DRIVER FOR KUBERNETES</li>
          <li class="breadcrumb-item">Container Storage Providers</li>
      <li class="breadcrumb-item active">HPE Alletra Storage MP B10000 File Service</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<div class="toc">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#platform_requirements">Platform Requirements</a></li>
<li><a href="#storageclass_parameters">StorageClass parameters</a></li>
<li><a href="#using_subpath_inline_nfs_and_static_nfs_persistent_volumes">Using subPath, Inline NFS and Static NFS Persistent Volumes</a><ul>
<li><a href="#subpath">subPath</a></li>
<li><a href="#inline_nfs">Inline NFS</a></li>
<li><a href="#static_nfs_persistent_volumes">Static NFS Persistent Volumes</a></li>
</ul>
</li>
<li><a href="#static_provisioning_for_orphaned_persistent_volumes">Static Provisioning for Orphaned Persistent Volumes</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#support">Support</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="platform_requirements">Platform Requirements<a class="headerlink" href="#platform_requirements" title="Permanent link">&para;</a></h2>
<p>The HPE Alletra Storage MP B10000 needs to be running OS 10.5 or later to take advantage of the File Service. Further, there needs to be IP ports configured on the array that are reachable from the Kubernetes compute nodes.</p>
<h2 id="storageclass_parameters">StorageClass parameters<a class="headerlink" href="#storageclass_parameters" title="Permanent link">&para;</a></h2>
<p>The CSP only supports dynamic provisioning of <code>PersistentVolumes</code> and no data management such as snapshot, cloning or converting block to file volumes.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>String</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>accessProtocol</td>
<td>Text</td>
<td>Mandatory, set to "nfs". Defaults to "iscsi" when unspecified.</td>
</tr>
<tr>
<td>destroyOnDelete<sup>1</sup></td>
<td>Boolean</td>
<td>Indicates the backing volume should be removed when the PVC is deleted. Defaults to "false" which means volumes needs to be pruned manually by a storage administrator. Defaults to "false".</td>
</tr>
<tr>
<td>shareSquashingOption</td>
<td>Text</td>
<td>Controls the NFS client UID to server UID mapping. Valid options: "no_root_squash", "root_squash", "all_squash". Defaults to "no_root_squash".</td>
</tr>
<tr>
<td>shareReadOnly</td>
<td>Boolean</td>
<td>Sets the server share read only. Defaults to "false".</td>
</tr>
<tr>
<td>shareNfsVersion</td>
<td>Float</td>
<td>For future use, Defaults to "4".</td>
</tr>
</tbody>
</table>
<p><small><sup>1</sup> = This parameter has no effect if the <code>PersistentVolume</code> being removed is empty (no user data).</small></p>
<p>Example default <code>StorageClass</code> (<a href="examples/storageclass.yaml">download</a>):</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    # There can only be one default StorageClass per cluster
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: hpe-standard-file
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/controller-expand-secret-name: hpe-file-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-file-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-file-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-file-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-file-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: Volume created by the HPE CSI Driver for Kubernetes
  accessProtocol: &quot;nfs&quot;
  destroyOnDelete: &quot;true&quot;
  shareSquashingOption: &quot;no_root_squash&quot;
  shareNfsVersion: &quot;4&quot;
  shareReadOnly: &quot;false&quot;
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
</code></pre></p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If all mutable parameters have values provided during provisioning of the <code>PersistentVolumes</code>, the <a href="../../using.html#using_volume_mutations">Volume Mutator</a> will later allow changes if needed.</p>
</div>
<h2 id="using_subpath_inline_nfs_and_static_nfs_persistent_volumes">Using subPath, Inline NFS and Static NFS Persistent Volumes<a class="headerlink" href="#using_subpath_inline_nfs_and_static_nfs_persistent_volumes" title="Permanent link">&para;</a></h2>
<p>Due to the constraints of the share count of the File Service on the array, there are a couple of workarounds that can be performed from the Kubernetes side to get better utilization.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The Inline NFS and Static NFS Persistent Volumes examples are provided to customers that prefer end-to-end static provisioning of storage resources. It's not recommended to refer to HPE CSI Driver provisioned file shares either inline or with static PVs as paths and client access restrictions may change in future versions of the CSI driver.</p>
</div>
<h3 id="subpath">subPath<a class="headerlink" href="#subpath" title="Permanent link">&para;</a></h3>
<p>Assume there is a file <code>PVC</code> created. In normal circumstances a handful of workloads in a <code>Namespace</code> would access the <code>PVC</code> as one shared filesystem. If a different workload in the <code>Namespace</code> would require a separate filesystem a new <code>PVC</code> would be the appropriate step to create separation. By using "subPath", workloads can be separated on the <code>PVC</code> by using directories, one sub directory for each workload.</p>
<div class="admonition tip">
<p class="admonition-title">Good to know</p>
<p>The "subPath" functionality is not provided by the HPE CSI Driver, it's a generic construct provided by Kubernetes.</p>
</div>
<p>Example:</p>
<p> <pre><code class=yaml>kind: Pod
apiVersion: v1
metadata:
  name: my-pod-sub
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
          subPath: nginx
    - name: pod-datelog-2
      image: debian
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
          subPath: debian
    - name: pod-datelog-3
      image: ubuntu
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
  volumes:
    - name: export1
      persistentVolumeClaim:
        claimName: my-first-pvc
</code></pre></p>
<p>Use <code>kubectl exec</code> to examine the directory structure in the third pod, without "subPath":</p>
<p> <pre><code class=text>$ kubectl exec my-pod-sub -c pod-datelog-3 -- find /data
/data
/data/nginx
/data/nginx/mydata.txt
/data/debian
/data/debian/mydata.txt
</code></pre></p>
<p>It's clear that the first and second <code>Pod</code> have different roots in the filesystem.</p>
<h3 id="inline_nfs">Inline NFS<a class="headerlink" href="#inline_nfs" title="Permanent link">&para;</a></h3>
<p>Inline NFS does NOT require the HPE CSI Driver, this procedure leverages the kubelet provide NFS client. It's expected that a share have been created on the array manually. Assume we have a filesystem volume named "volume" and the share name is "example" on the share IP address of 192.168.1.100.</p>
<p> <pre><code class=yaml>kind: Pod
apiVersion: v1
metadata:
  name: my-pod-inline
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
  volumes:
    - name: export1
      nfs:
        server: 192.168.1.100
        path: /file/volume/example
</code></pre></p>
<h3 id="static_nfs_persistent_volumes">Static NFS Persistent Volumes<a class="headerlink" href="#static_nfs_persistent_volumes" title="Permanent link">&para;</a></h3>
<p>Similar to inline NFS, static NFS <code>PVs</code> does not require the HPE CSI Driver. The file share needs to exist prior.</p>
<p>Create a static <code>PV</code> referencing an existing file share on the array:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-nfs-pv
spec:
  capacity:
    storage: 8Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: &quot;&quot;
  nfs:
    path: /file/volume/example
    server: 192.168.1.100
</code></pre></p>
<p>Next, create a <code>PVC</code> referencing the <code>PV</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 8Gi
  volumeName: my-nfs-pv
  storageClassName: &quot;&quot;
</code></pre></p>
<p>Workloads can now reference the <code>PVC</code> as usual.</p>
<h2 id="static_provisioning_for_orphaned_persistent_volumes">Static Provisioning for Orphaned Persistent Volumes<a class="headerlink" href="#static_provisioning_for_orphaned_persistent_volumes" title="Permanent link">&para;</a></h2>
<p>In the event of a file share becoming an orphan on the array due to "destroyOnDelete" not being set properly, it's possible to statically define the <code>PersistentVolume</code> (<code>PV</code>) and claim the <code>PV</code> manually.</p>
<p>First, a <code>PV</code> needs to be created manually referencing the existing file share.</p>
<p>This table describes how to retrieve the attributes needed.</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>metadata.name</td>
<td>pvc-UUID</td>
<td>This is referenced on the array as the file share name.</td>
</tr>
<tr>
<td>spec.capacity.storage</td>
<td>Size in GiB</td>
<td>This must match the size of the existing file share filesystem.</td>
</tr>
<tr>
<td>spec.csi.volumeHandle</td>
<td>referenceUid</td>
<td>This value can be retrieved from URL in the web UI of the array while viewing the file share.</td>
</tr>
<tr>
<td>spec.csi.volumeAttributes.mountPath</td>
<td>Path</td>
<td>This is referenced on the array as path.</td>
</tr>
</tbody>
</table>
<p>Replace the zeroed fields below with the actual values from the file share on the array.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pvc-00000000-0000-0000-0000-000000000000
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 22Gi
  csi:
    volumeHandle: 00000000000000000000000000000000
    driver: csi.hpe.com
    volumeAttributes:
      accessProtocol: nfs
      volumeAccessMode: mount
      mountPath: /file/pvc-00000000-0000-0000-0000-000000000000/pvc-00000000-0000-0000-0000-000000000000
    controllerPublishSecretRef:
      name: hpe-file-backend
      namespace: hpe-storage
    nodePublishSecretRef:
      name: hpe-file-backend
      namespace: hpe-storage
    controllerExpandSecretRef:
      name: hpe-file-backend
      namespace: hpe-storage
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
</code></pre></p>
<p>Next, create a <code>PVC</code> referencing the <code>PV</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-static-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 22Gi
  volumeName: pvc-00000000-0000-0000-0000-000000000000
  storageClassName: &quot;&quot;
</code></pre></p>
<p>The <code>PVC</code> may now be referenced and attached to a workload.</p>
<h2 id="limitations">Limitations<a class="headerlink" href="#limitations" title="Permanent link">&para;</a></h2>
<p>These are the current limitations of the HPE Alletra Storage MP B10000 File Service CSP.</p>
<ul>
<li>Snapshots and clones are not yet implemented.</li>
<li>Maximum 16 File Service shares may exist at any given time per array controller.</li>
<li>Maximum 20% of the array provisioned capacity may be used for File Service storage.</li>
<li>Maximum 64TiB capacity per file share.</li>
<li>Export permissions on the array are set to <code>*</code> (defined as "all" in the array interfaces) and will be reachable from any host that can reach the File interfaces on the array.</li>
</ul>
<h2 id="support">Support<a class="headerlink" href="#support" title="Permanent link">&para;</a></h2>
<p>Please refer to the HPE Alletra Storage MP B10000, Alletra 9000 and Primera and 3PAR Storage CSP <a href="../../../legal/support/index.html#hpe_alletra_storage_mp_b10000_alletra_9000_and_primera_and_3par_container_storage_provider_support">support statement</a>.</p>
              
            </div>
          </div>

<footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2020-2025 Hewlett Packard Enterprise Development LP<br />Give <a href="https://github.com/hpe-storage/scod/issues/new?title=csi_driver/container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/hpe-storage/scod" class="fa fa-code-fork" style="color: #fcfcfc"> hpe-storage/scod</a>
        </span>
    
    
      <span><a href="../hpe_alletra_storage_mp_b10000/index.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../hpe_alletra_6000/index.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
