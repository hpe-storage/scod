<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Hewlett Packard Enterprise" /><link rel="canonical" href="https://scod.hpedev.io/csi_driver/using.html" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="HPE Storage Container Orchestrator Documentation (SCOD)"/>
    <meta property="og:description" content="This is an umbrella documentation project for all container integrations surrounding HPE block, file and object storage. Tailored for IT ops, developers and technology partners."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://scod.hpedev.io"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://scod.hpedev.io/img/hpe-social-og-image01.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
    
    <title>Using - SCOD.HPEDEV.IO</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/hpedev.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Using";
        var mkdocs_page_input_path = "csi_driver/using.md";
        var mkdocs_page_url = "/csi_driver/using.html";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-PC28RTKKTW"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', "G-PC28RTKKTW");
      </script>
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="..">
          <img src="../img/hpe.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE CSI DRIVER FOR KUBERNETES</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Using</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#pvc_access_modes">PVC Access Modes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#enabling_csi_snapshots">Enabling CSI Snapshots</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#base_storageclass_parameters">Base StorageClass Parameters</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#enabling_iscsi_chap">Enabling iSCSI CHAP</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cluster-wide_iscsi_chap_credentials">Cluster-wide iSCSI CHAP Credentials</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#per_storageclass_iscsi_chap_credentials">Per StorageClass iSCSI CHAP Credentials</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#provisioning_concepts">Provisioning Concepts</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#create_a_persistentvolumeclaim_from_a_storageclass">Create a PersistentVolumeClaim from a StorageClass</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ephemeral_inline_volumes">Ephemeral Inline Volumes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#raw_block_volumes">Raw Block Volumes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_csi_snapshots">Using CSI Snapshots</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#volume_groups">Volume Groups</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#snapshot_groups">Snapshot Groups</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#expanding_pvcs">Expanding PVCs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_pvc_overrides">Using PVC Overrides</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_volume_mutations">Using Volume Mutations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_the_nfs_server_provisioner">Using the NFS Server Provisioner</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#using_a_foreign_storageclass">Using a Foreign StorageClass</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#limitations_and_considerations_for_the_nfs_server_provisioner">Limitations and Considerations for the NFS Server Provisioner</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using_volume_encryption">Using Volume Encryption</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#topology_and_volumebindingmode">Topology and volumeBindingMode</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#label_compute_nodes">Label Compute Nodes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#create_storageclass_with_topology_information">Create StorageClass with Topology Information</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#static_provisioning">Static Provisioning</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#further_reading">Further Reading</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Container Storage Providers</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="container_storage_provider/hpe_alletra_storage_mp_b10000/index.html">HPE Alletra Storage MP B10000, Alletra 9000, Primera and 3PAR</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.html">HPE Alletra Storage MP B10000 File Service</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="container_storage_provider/hpe_alletra_6000/index.html">HPE Alletra 5000/6000 and Nimble Storage</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="monitor.html">Pod Monitor</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="operations.html">Auxiliary Operations</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="diagnostics.html">Diagnostics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Partner Ecosystems</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="partners/hpe_morpheus/install.html">HPE Morpheus Kubernetes Service</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/hpe_ezmeral/install.html">HPE Ezmeral Runtime Enterprise</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/amazon_eks_anywhere/index.html">Amazon EKS Anywhere</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/canonical/index.html">Canonical</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/cohesity/index.html">Cohesity</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/commvault/index.html">Commvault</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/kasten/index.html">Veeam Kasten</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/mirantis/index.html">Mirantis</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/redhat_openshift/index.html">Red Hat OpenShift</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/suse_virtualization/index.html">SUSE Virtualization</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/suse_rancher/index.html">SUSE Rancher</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/tkgi/index.html">Tanzu TKGI</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="partners/vmware/index.html">VMware</a>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE COSI DRIVER FOR KUBERNETES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../cosi_driver/index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cosi_driver/deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cosi_driver/using.html">Using</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cosi_driver/diagnostics.html">Diagnostics</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPE GREENLAKE FOR FILE STORAGE</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../filex_csi_driver/index.html">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../filex_csi_driver/deployment.html">Deployment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../filex_csi_driver/using.html">Using</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEARN</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../learn/persistent_storage/index.html">Persistent Storage for Kubernetes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../learn/introduction_to_containers/index.html">For HPE partners:<br />&nbsp;&nbsp; Introduction to Containers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../learn/csi_primitives/index.html">Introduction to CSI Primitives</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../learn/video_gallery/index.html">Video Gallery</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../learn/csi_workshop/index.html">Interactive CSI Workshop</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">COMMUNITY</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://developer.hpe.com">HPE Developer</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://slack.hpedev.io">Sign up to HPE Developer on Slack</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage/scod/issues/new?title=I have some feedback on SCOD">Got feedback?</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EXTERNAL LINKS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage/scod">SCOD on GitHub</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/hpe-storage">HPE Storage on GitHub</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://hpe.com/storage/containers">Storage for Containers on hpe.com</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/support/index.html">Support</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/notices/index.html">Notices</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGACY DRIVERS AND PLUGINS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legacy/index.html">Docker, FlexVolume and CSPs</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">SCOD.HPEDEV.IO</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">HPE CSI DRIVER FOR KUBERNETES</li>
      <li class="breadcrumb-item active">Using</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h1>
<p>At this point the CSI driver and CSP should be installed and configured.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Most examples below assumes there's a <code>Secret</code> named "hpe-backend" in the "hpe-storage" <code>Namespace</code>. Learn how to add <code>Secrets</code> in the <a href="deployment.html#add_an_hpe_storage_backend">Deployment section</a>.</p>
</div>
<div class="toc">
<ul>
<li><a href="#overview">Overview</a><ul>
<li><a href="#pvc_access_modes">PVC Access Modes</a></li>
<li><a href="#enabling_csi_snapshots">Enabling CSI Snapshots</a></li>
<li><a href="#base_storageclass_parameters">Base StorageClass Parameters</a></li>
<li><a href="#enabling_iscsi_chap">Enabling iSCSI CHAP</a><ul>
<li><a href="#cluster-wide_iscsi_chap_credentials">Cluster-wide iSCSI CHAP Credentials</a></li>
<li><a href="#per_storageclass_iscsi_chap_credentials">Per StorageClass iSCSI CHAP Credentials</a></li>
</ul>
</li>
<li><a href="#provisioning_concepts">Provisioning Concepts</a><ul>
<li><a href="#create_a_persistentvolumeclaim_from_a_storageclass">Create a PersistentVolumeClaim from a StorageClass</a></li>
<li><a href="#ephemeral_inline_volumes">Ephemeral Inline Volumes</a></li>
<li><a href="#raw_block_volumes">Raw Block Volumes</a></li>
<li><a href="#using_csi_snapshots">Using CSI Snapshots</a></li>
<li><a href="#volume_groups">Volume Groups</a></li>
<li><a href="#snapshot_groups">Snapshot Groups</a></li>
<li><a href="#expanding_pvcs">Expanding PVCs</a></li>
<li><a href="#using_pvc_overrides">Using PVC Overrides</a></li>
<li><a href="#using_volume_mutations">Using Volume Mutations</a></li>
<li><a href="#using_the_nfs_server_provisioner">Using the NFS Server Provisioner</a><ul>
<li><a href="#using_a_foreign_storageclass">Using a Foreign StorageClass</a><ul>
<li><a href="#example_storageclass_using_a_foreign_storageclass">Example StorageClass using a foreign StorageClass</a></li>
</ul>
</li>
<li><a href="#limitations_and_considerations_for_the_nfs_server_provisioner">Limitations and Considerations for the NFS Server Provisioner</a></li>
</ul>
</li>
<li><a href="#using_volume_encryption">Using Volume Encryption</a></li>
<li><a href="#topology_and_volumebindingmode">Topology and volumeBindingMode</a><ul>
<li><a href="#label_compute_nodes">Label Compute Nodes</a></li>
<li><a href="#create_storageclass_with_topology_information">Create StorageClass with Topology Information</a></li>
</ul>
</li>
<li><a href="#static_provisioning">Static Provisioning</a></li>
</ul>
</li>
<li><a href="#further_reading">Further Reading</a></li>
</ul>
</li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you're familiar with the basic concepts of persistent storage on Kubernetes and are looking for an overview of example YAML declarations for different object types supported by the HPE CSI driver, <a href="https://github.com/hpe-storage/csi-driver/tree/master/examples/kubernetes">visit the source code repo</a> on GitHub.</p>
</div>
<h2 id="pvc_access_modes">PVC Access Modes<a class="headerlink" href="#pvc_access_modes" title="Permanent link">&para;</a></h2>
<p>An application that requires persistent or ephemeral storage from a CSI driver usually provide guidance to the user on what kind of storage access mode is needed for the <code>PVCs</code>. Below are the most common distinctions.</p>
<table>
<thead>
<tr>
<th>Access Mode</th>
<th>Abbreviation</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReadWriteOnce</td>
<td>RWO</td>
<td>For high performance <code>Pods</code> where access to the PVC is exclusive to one host at a time.</td>
</tr>
<tr>
<td>ReadWriteOncePod</td>
<td>RWOP</td>
<td>Exclusive access by a single <code>Pod</code>. Not currently supported by the HPE CSI Driver.</td>
</tr>
<tr>
<td>ReadWriteMany</td>
<td>RWX</td>
<td>For shared filesystems where multiple <code>Pods</code> in the same <code>Namespace</code> need simultaneous access to a PVC across multiple nodes.</td>
</tr>
<tr>
<td>ReadOnlyMany</td>
<td>ROX</td>
<td>Read-only representation of RWX.</td>
</tr>
</tbody>
</table>
<p>The HPE CSI Driver for Kubernetes is a block and file storage CSI driver. The <code>PVC</code> <code>accessMode</code> is also dictated by the capability of the backend CSP intersected with the <code>volumeMode</code> being requested.</p>
<table>
 <thead>
  <tr>
   <th>Volume Mode</th>
   <th>CSP Type</th>
   <th>Access Mode</th>
  </tr>
 </thead>
 <tr>
  <td>Block</td>
  <td>Block</td>
  <td>RWO, RWX, ROX</td>
 </tr>
 <tr>
  <td rowspan="2">Filesystem</td>
  <td>Block</td>
  <td>RWO. RWX and ROX through NFS Server Provisioner<sup>1</sup></td>
 </tr>
 <tr>
  <td>File</td>
  <td>RWO, RWX, ROX</td>
 </tr>
</table>

<p><small><sup>1</sup> = The most common pattern for cloud-native workloads is using <code>volumeMode: Filesystem</code> (the default if not specified in the <code>PVC</code>) with <code>ReadWriteOnce</code> (RWO) access mode from a block CSP. <code>ReadWriteMany</code> (RWX) and <code>ReadOnlyMany</code> (ROX) may use either a file CSP or the NFS Server Provisioner. The NFS Server Provisioner is enabled by <a href="#using_the_nfs_server_provisioner">transparently deploying a NFS server</a> for each Persistent Volume Claim (PVC) against a <code>StorageClass</code> where it's enabled, that in turn is backed by a traditional RWO claim. Most of the examples featured on SCOD are illustrated as RWO using block based storage with <code>volumeMode: Filesystem</code>, but many of the examples apply in the majority of use cases.</small></p>
<div class="admonition seealso">
<p class="admonition-title">ReadWriteOnce and access by multiple Pods</p>
<p><code>Pods</code> that require access to the same "ReadWriteOnce" (RWO) PVC needs to reside on the same node and <code>Namespace</code> by using <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">selectors</a> or <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity scheduling rules</a> applied when deployed. If not configured correctly, the <code>Pod</code> will fail to start and will throw a "Multi-Attach" error in the event log if the PVC is already attached to a <code>Pod</code> that has been scheduled on a different node within the cluster.</p>
</div>
<h2 id="enabling_csi_snapshots">Enabling CSI Snapshots<a class="headerlink" href="#enabling_csi_snapshots" title="Permanent link">&para;</a></h2>
<p>Support for <code>VolumeSnapshotClasses</code> and <code>VolumeSnapshots</code> is available from Kubernetes 1.17+. The snapshot CRDs and the common snapshot controller needs to be installed manually. As per Kubernetes TAG Storage, these should not be installed as part of a CSI driver and should be deployed by the Kubernetes cluster vendor or user.</p>
<p>Ensure the snapshot CRDs and common snapshot controller hasn't been installed already.</p>
<p> <pre><code class=text>kubectl get crd volumesnapshots.snapshot.storage.k8s.io \
  volumesnapshotcontents.snapshot.storage.k8s.io \
  volumesnapshotclasses.snapshot.storage.k8s.io
</code></pre></p>
<p>Vendors may package, name and deploy the common snapshot controller using their own naming conventions. Run the command below and look for workload names that contain "snapshot".</p>
<p> <pre><code class=text>kubectl get sts,deploy -A
</code></pre></p>
<p>If no prior CRDs or controllers exist, install the snapshot CRDs and common snapshot controller (once per Kubernetes cluster, independent of any CSI drivers).</p>
<div class=md-fenced-code-tabs id=tab-tab-group-2><input name=tab-group-2 type=radio id=tab-group-2-0_text checked=checked class=code-tab data-lang=text aria-controls=tab-group-2-0_text-panel role=tab><label for=tab-group-2-0_text class=code-tab-label data-lang=text id=tab-group-2-0_text-label>HPE CSI Driver v3.0.1</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-2-0_text-panel aria-labelledby=tab-group-2-0_text-label><pre><code class=text># Kubernetes 1.30-1.33
git clone https://github.com/kubernetes-csi/external-snapshotter
cd external-snapshotter
git checkout tags/v8.2.0 -b hpe-csi-driver-v3.0.1
kubectl kustomize client/config/crd | kubectl create -f-
kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f-
</code></pre></div><input name=tab-group-2 type=radio id=tab-group-2-1_text class=code-tab data-lang=text aria-controls=tab-group-2-1_text-panel role=tab><label for=tab-group-2-1_text class=code-tab-label data-lang=text id=tab-group-2-1_text-label>v2.5.2</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-2-1_text-panel aria-labelledby=tab-group-2-1_text-label><pre><code class=text># Kubernetes 1.29-1.32
git clone https://github.com/kubernetes-csi/external-snapshotter
cd external-snapshotter
git checkout tags/v8.2.0 -b hpe-csi-driver-v2.5.2
kubectl kustomize client/config/crd | kubectl create -f-
kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f-
</code></pre></div><input name=tab-group-2 type=radio id=tab-group-2-2_text class=code-tab data-lang=text aria-controls=tab-group-2-2_text-panel role=tab><label for=tab-group-2-2_text class=code-tab-label data-lang=text id=tab-group-2-2_text-label>v2.5.0</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-2-2_text-panel aria-labelledby=tab-group-2-2_text-label><pre><code class=text># Kubernetes 1.27-1.30
git clone https://github.com/kubernetes-csi/external-snapshotter
cd external-snapshotter
git checkout tags/v8.0.1 -b hpe-csi-driver-v2.5.0
kubectl kustomize client/config/crd | kubectl create -f-
kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f-
</code></pre></div><input name=tab-group-2 type=radio id=tab-group-2-3_text class=code-tab data-lang=text aria-controls=tab-group-2-3_text-panel role=tab><label for=tab-group-2-3_text class=code-tab-label data-lang=text id=tab-group-2-3_text-label>v2.4.2</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-2-3_text-panel aria-labelledby=tab-group-2-3_text-label><pre><code class=text># Kubernetes 1.26-1.29
git clone https://github.com/kubernetes-csi/external-snapshotter
cd external-snapshotter
git checkout tags/v6.3.3 -b hpe-csi-driver-v2.4.2
kubectl kustomize client/config/crd | kubectl create -f-
kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f-
</code></pre></div></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <a href="#provisioning_concepts">provisioning</a> section contains examples on how to create <code>VolumeSnapshotClass</code> and <code>VolumeSnapshot</code> objects.</p>
</div>
<h2 id="base_storageclass_parameters">Base StorageClass Parameters<a class="headerlink" href="#base_storageclass_parameters" title="Permanent link">&para;</a></h2>
<p>Each CSP has its own set of unique parameters to control the provisioning behavior. These examples serve as a base <code>StorageClass</code> example for each version of Kubernetes. See the respective <a href="container_storage_provider/index.html">CSP</a> for more elaborate examples.</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: hpe-standard
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: &quot;Volume created by the HPE CSI Driver for Kubernetes&quot;
reclaimPolicy: Delete
allowVolumeExpansion: true
</code></pre></p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Replace "hpe-backend" with a <code>Secret</code> relevant to the backend being referenced.<br /></p>
</div>
<p>Common HPE CSI Driver <code>StorageClass</code> parameters across CSPs.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>String</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>accessProtocol</td>
<td>Text</td>
<td>The access protocol to use when accessing the persistent volume ("nfs", "fc" or "iscsi").  Default: "iscsi"</td>
</tr>
<tr>
<td>chapSecretName</td>
<td>Text</td>
<td>Name of <code>Secret</code> to use for iSCSI CHAP.</td>
</tr>
<tr>
<td>chapSecretNamespace</td>
<td>Text</td>
<td>Namespace of <code>Secret</code> to use for iSCSI CHAP.</td>
</tr>
<tr>
<td>description<sup>1</sup></td>
<td>Text</td>
<td>Text to be added to the volume PV metadata on the backend CSP. Default: ""</td>
</tr>
<tr>
<td>csi.storage.k8s.io/fstype</td>
<td>Text</td>
<td>Filesystem to format new volumes with. XFS is preferred, ext3, ext4 and btrfs is supported. Defaults to "ext4" if omitted.</td>
</tr>
<tr>
<td>fsOwner</td>
<td>userId:groupId</td>
<td>The user id and group id that should own the root directory of the filesystem.</td>
</tr>
<tr>
<td>fsMode</td>
<td>Octal digits</td>
<td>1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem.</td>
</tr>
<tr>
<td>fsCreateOptions</td>
<td>Text</td>
<td>A string to be passed to the mkfs command.  These flags are opaque to CSI and are therefore not validated.  To protect the node, only the following characters are allowed:  <code>[a-zA-Z0-9=, \-]</code>.</td>
</tr>
<tr>
<td>fsRepair</td>
<td>Boolean</td>
<td>When set to "true", if a mount fails and filesystem corruption is detected, this parameter will control if an actual repair will be attempted. Default: "false". <br/><br /><strong>Note:</strong> <code>fsRepair</code> is unable to detect or remedy corrupted filesystems that are already mounted. <strong>Data loss may occur during the attempt to repair the filesystem.</strong></td>
</tr>
<tr>
<td>nfsResources</td>
<td>Boolean</td>
<td>When set to "true", requests against the <code>StorageClass</code> will create resources for the NFS Server Provisioner (<code>Deployment</code>, RWO <code>PVC</code> and <code>Service</code>). Required parameter for ReadWriteMany and ReadOnlyMany accessModes. Default: "false"</td>
</tr>
<tr>
<td>nfsForeignStorageClass</td>
<td>Text</td>
<td>Provision NFS servers on <code>PVCs</code> from a different <code>StorageClass</code>. See <a href="#using_a_foreign_storageclass">Using a Foreign StorageClass</a></td>
</tr>
<tr>
<td>nfsNamespace</td>
<td>Text</td>
<td>Resources are by default created in the "hpe-nfs" <code>Namespace</code>. If CSI <code>VolumeSnapshotClass</code> and <code>dataSource</code> functionality is required on the requesting claim, requesting and backing PVC need to exist in the requesting <code>Namespace</code>. A value of "csi.storage.k8s.io/pvc/namespace" will provision resources in the requesting <code>PVC</code> <code>Namespace</code>.</td>
</tr>
<tr>
<td>nfsNodeSelector</td>
<td>Text</td>
<td>Customize the <code>nodeSelector</code> label value for the NFS <code>Pod</code>. The default behavior is to omit the <code>nodeSelector</code>.</td>
</tr>
<tr>
<td>nfsMountOptions</td>
<td>Text</td>
<td>Customize NFS mount options for the <code>Pods</code> to the server <code>Deployment</code>. Uses <code>mount</code> command defaults from the node.</td>
</tr>
<tr>
<td>nfsProvisionerImage</td>
<td>Text</td>
<td>Customize provisioner image for the server <code>Deployment</code>. Default: Official build from "hpestorage/nfs-provisioner" repo</td>
</tr>
<tr>
<td>nfsResourceRequestsCpuM</td>
<td>Text</td>
<td>Specify CPU requests for the server <code>Deployment</code> in milli CPU. Default: "500m". Example: "4000m". Set to "0" to disable.</td>
</tr>
<tr>
<td>nfsResourceRequestsMemoryMi</td>
<td>Text</td>
<td>Specify memory requests (in megabytes) for the server <code>Deployment</code>. Default: "512Mi". Example: "4096Mi". Set to "0" to disable.</td>
</tr>
<tr>
<td>nfsResourceLimitsCpuM</td>
<td>Text</td>
<td>Specify CPU limits for the server <code>Deployment</code> in milli CPU. Default: "1000m". Example: "4000m". Set to "0" to disable.</td>
</tr>
<tr>
<td>nfsResourceLimitsMemoryMi</td>
<td>Text</td>
<td>Specify memory limits (in megabytes) for the server <code>Deployment</code>. Default: "2048Mi". Example: "500Mi". Recommended minimum: "2048Mi". Set to "0" to disable.</td>
</tr>
<tr>
<td>nfsTolerationSeconds</td>
<td>Text</td>
<td>Change the milestone wait time for an unresponsive host. Default: "30"</td>
</tr>
<tr>
<td>hostEncryption</td>
<td>Boolean</td>
<td>Direct the CSI driver to invoke Linux Unified Key Setup (LUKS) via the <code>dm-crypt</code> kernel module. Default: "false". See <a href="#using_volume_encryption">Volume encryption</a> to learn more.</td>
</tr>
<tr>
<td>hostEncryptionSecretName</td>
<td>Text</td>
<td>Name of the <code>Secret</code> to use for the volume encryption. Mandatory if "hostEncryption" is enabled. Default: ""</td>
</tr>
<tr>
<td>hostEncryptionSecretNamespace</td>
<td>Text</td>
<td><code>Namespace</code> where to find "hostEncryptionSecretName". Default: ""</td>
</tr>
</tbody>
</table>
<p><small><sup>1</sup> = Parameter is mutable using the <a href="#using_volume_mutations">CSI Volume Mutator</a>.</small></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All common HPE CSI Driver parameters are optional.</p>
</div>
<h2 id="enabling_iscsi_chap">Enabling iSCSI CHAP<a class="headerlink" href="#enabling_iscsi_chap" title="Permanent link">&para;</a></h2>
<p>Familiarize yourself with the <a href="index.html#iscsi_chap_considerations">iSCSI CHAP Considerations</a> before proceeding. This section describes how to enable iSCSI CHAP with HPE CSI Driver 2.5.0 and later.</p>
<p>Create an iSCSI CHAP <code>Secret</code>. The referenced CHAP account does not need to exist on the storage backend, it will be created by the CSP if it doesn't exist.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Secret
metadata:
  name: my-chap-secret
  namespace: hpe-storage
stringData:
  # Up to 64 characters including \-:., must start with an alpha-numeric character.
  chapUser: &quot;my-chap-user&quot;
  # Between 12 to 16 alpha-numeric characters.
  chapPassword: &quot;my-chap-password&quot;
</code></pre></p>
<p>Once the <code>Secret</code> has been created, there are two methods available to use it depending on the situation, cluster-wide or per <code>StorageClass</code>.</p>
<h3 id="cluster-wide_iscsi_chap_credentials">Cluster-wide iSCSI CHAP Credentials<a class="headerlink" href="#cluster-wide_iscsi_chap_credentials" title="Permanent link">&para;</a></h3>
<p>The cluster-wide iSCSI CHAP credentials will be used by all iSCSI-based <code>PersistentVolumes</code> regardless of backend and <code>StorageClass</code>. The CHAP <code>Secret</code> is simply referenced during install of the HPE CSI Driver for Kubernetes Helm Chart. The <code>Secret</code> and <code>Namespace</code> needs to exist prior to install.</p>
<p>Example:</p>
<p> <pre><code class=text>helm install my-hpe-csi-driver -n hpe-storage \
  hpe-storage/hpe-csi-driver \
  --set iscsi.chapSecretName=my-chap-secret
</code></pre></p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Once a <code>PersistentVolume</code> has been provisioned with cluster-wide iSCSI CHAP credentials it's not possible to switch over to per <code>StorageClass</code> iSCSI CHAP credentials.<br /><br />If CSI driver 2.4.2 or earlier has been used, cluster-wide iSCSI CHAP credentials is the only way to provide the credentials for volumes provisioned with 2.4.2 or earlier.</p>
</div>
<h3 id="per_storageclass_iscsi_chap_credentials">Per StorageClass iSCSI CHAP Credentials<a class="headerlink" href="#per_storageclass_iscsi_chap_credentials" title="Permanent link">&para;</a></h3>
<p>The CHAP <code>Secret</code> may be referenced in a <code>StorageClass</code>.</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: hpe-standard
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: &quot;Volume created by the HPE CSI Driver for Kubernetes&quot;
  chapSecretNamespace: hpe-storage
  chapSecretName: my-chap-secret
reclaimPolicy: Delete
allowVolumeExpansion: true
</code></pre></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The iSCSI CHAP credentials are in reality per iSCSI Target. Do <strong>NOT</strong> create multiple <code>StorageClasses</code> referencing different CHAP <code>Secrets</code> with different credentials for the same backend. It will result in a data outage with conflicting sessions.<br /><br />Ensure the same <code>Secret</code> is referenced in all <code>StorageClasses</code> using a particular backend.</p>
</div>
<h2 id="provisioning_concepts">Provisioning Concepts<a class="headerlink" href="#provisioning_concepts" title="Permanent link">&para;</a></h2>
<p>These instructions are provided as an example on how to use the HPE CSI Driver with a <a href="container_storage_provider/index.html">CSP</a> supported by HPE.</p>
<ul>
<li><a href="#create_a_persistentvolumeclaim_from_a_storageclass">Create a PersistentVolumeClaim from a StorageClass</a></li>
<li><a href="#ephemeral_inline_volumes">Ephemeral inline volumes</a></li>
<li><a href="#raw_block_volumes">Raw Block Volumes</a></li>
<li><a href="#using_csi_snapshots">Using CSI Snapshots</a></li>
<li><a href="#volume_groups">Volume Groups</a></li>
<li><a href="#snapshot_groups">Snapshot Groups</a></li>
<li><a href="#expanding_pvcs">Expanding PVCs</a></li>
<li><a href="#using_pvc_overrides">Using PVC Overrides</a></li>
<li><a href="#using_volume_mutations">Using Volume Mutations</a></li>
<li><a href="#using_volume_encryption">Using Volume Encryption</a></li>
<li><a href="#using_the_nfs_server_provisioner">Using the NFS Server Provisioner</a></li>
<li><a href="#using_volume_encryption">Using volume encryption</a></li>
<li><a href="#topology_and_volumebindingmode">Topology and volumeBindingMode</a></li>
<li><a href="#static_provisioning">Static Provisioning</a></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">New to Kubernetes?</p>
<p>There's a basic tutorial of how dynamic provisioning of persistent storage on Kubernetes works in the <a href="../learn/video_gallery/index.html#dynamic_provisioning_of_persistent_storage_on_kubernetes">Video Gallery</a>.</p>
</div>
<h3 id="create_a_persistentvolumeclaim_from_a_storageclass">Create a PersistentVolumeClaim from a StorageClass<a class="headerlink" href="#create_a_persistentvolumeclaim_from_a_storageclass" title="Permanent link">&para;</a></h3>
<p>The below YAML declarations are meant to be created with <code>kubectl create</code>. Either copy the content to a file on the host where <code>kubectl</code> is being executed, or copy &amp; paste into the terminal, like this:</p>
<p> <pre><code class=text>kubectl create -f-
&lt; paste the YAML &gt;
^D (CTRL + D)
</code></pre></p>
<p>To get started, create a <code>StorageClass</code> API object referencing the CSI driver <code>Secret</code> relevant to the backend.</p>
<p>These examples are for Kubernetes 1.15+</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-scod
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: &quot;Volume created by the HPE CSI Driver for Kubernetes&quot;
  accessProtocol: iscsi
reclaimPolicy: Delete
allowVolumeExpansion: true
</code></pre></p>
<p>Create a <code>PersistentVolumeClaim</code>. This object declaration ensures a <code>PersistentVolume</code> is created and provisioned on your behalf, make sure to reference the correct <code>.spec.storageClassName</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
  storageClassName: hpe-scod
</code></pre></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In most environments, there is a default <code>StorageClass</code> declared on the cluster. In such a scenario, the <code>.spec.storageClassName</code> can be omitted. The default <code>StorageClass</code> is controlled by an annotation: <code>.metadata.annotations.storageclass.kubernetes.io/is-default-class</code> set to either <code>"true"</code> or <code>"false"</code>.</p>
</div>
<p>After the <code>PersistentVolumeClaim</code> has been declared, check that a new <code>PersistentVolume</code> is created based on your claim:</p>
<p> <pre><code class=text>kubectl get pv
NAME              CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM                STORAGECLASS AGE
pvc-13336da3-7... 32Gi     RWO          Delete         Bound  default/my-first-pvc hpe-scod     3s
</code></pre></p>
<p>The above output means that the HPE CSI Driver successfully provisioned a new volume. The volume is not attached to any node yet. It will only be attached to a node if a scheduled workload requests the <code>PersistentVolumeClaim</code>. Now, let us create a <code>Pod</code> that refers to the above volume. When the <code>Pod</code> is created, the volume will be attached, formatted and mounted according to the specification.</p>
<p> <pre><code class=yaml>kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
    - name: pod-datelog-2
      image: debian
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
  volumes:
    - name: export1
      persistentVolumeClaim:
        claimName: my-first-pvc
</code></pre></p>
<p>Check if the <code>Pod</code> is running successfully.</p>
<p> <pre><code class=text>kubectl get pod my-pod
NAME        READY   STATUS    RESTARTS   AGE
my-pod      2/2     Running   0          2m29s
</code></pre></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A simple <code>Pod</code> does not provide any automatic recovery if the node the <code>Pod</code> is scheduled on crashes or become unresponsive. Please see <a href="https://kubernetes.io/docs/concepts/workloads/">the official Kubernetes documentation</a> for different workload types that provide automatic recovery. A shortlist of recommended workload types that are suitable for persistent storage is available in <a href="https://datamattsson.tumblr.com/post/182297931146/highly-available-stateful-workloads-on-kubernetes">this blog post</a> and best practices are outlined in <a href="https://datamattsson.tumblr.com/post/185031432701/best-practices-for-stateful-workloads-on">this blog post</a>.</p>
</div>
<h3 id="ephemeral_inline_volumes">Ephemeral Inline Volumes<a class="headerlink" href="#ephemeral_inline_volumes" title="Permanent link">&para;</a></h3>
<p>It's possible to declare a volume "inline" a <code>Pod</code> specification. The volume is ephemeral and only persists as long as the <code>Pod</code> is running. If the <code>Pod</code> gets rescheduled, deleted or upgraded, the volume is deleted and a new volume gets provisioned if it gets restarted.</p>
<p>Ephemeral inline volumes are not associated with a <code>StorageClass</code>, hence a <code>Secret</code> needs to be provided inline with the volume.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Allowing user <code>Pods</code> to access the CSP <code>Secret</code> gives them the same privileges on the backend system as the HPE CSI Driver.</p>
</div>
<p>There are two ways to declare the <code>Secret</code> with ephemeral inline volumes, either the <code>Secret</code> is in the same <code>Namespace</code> as the workload being declared or it resides in a foreign <code>Namespace</code>.</p>
<p>Local <code>Secret</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Pod
metadata:
  name: my-pod-inline-mount-1
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: my-volume-1
          mountPath: /data
  volumes:
    - name: my-volume-1
      csi:
       driver: csi.hpe.com
       nodePublishSecretRef:
         name: hpe-backend
       fsType: ext3
       volumeAttributes:
         csi.storage.k8s.io/ephemeral: &quot;true&quot;
         accessProtocol: &quot;iscsi&quot;
         size: &quot;5Gi&quot;
</code></pre></p>
<p>Foreign <code>Secret</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Pod
metadata:
  name: my-pod-inline-mount-2
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: my-volume-1
          mountPath: /data
  volumes:
    - name: my-volume-1
      csi:
       driver: csi.hpe.com
       fsType: ext3
       volumeAttributes:
         csi.storage.k8s.io/ephemeral: &quot;true&quot;
         inline-volume-secret-name: hpe-backend
         inline-volume-secret-namespace: hpe-storage
         accessProtocol: &quot;iscsi&quot;
         size: &quot;7Gi&quot;
</code></pre></p>
<p>The parameters used in the examples are the bare minimum required parameters. Any parameters supported by the HPE CSI Driver and backend CSP may be used for ephemeral inline volumes. See the <a href="#base_storageclass_parameters">base StorageClass parameters</a> or the respective CSP being used.</p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>For more elaborate use cases around ephemeral inline volumes, check out the tutorial on HPE Developer: <a href="https://developer.hpe.com/blog/EE2QnZBXXwi4o7X0E4M0/using-raw-block-and-ephemeral-inline-volumes-on-kubernetes">Using Ephemeral Inline Volumes on Kubernetes</a></p>
</div>
<h3 id="raw_block_volumes">Raw Block Volumes<a class="headerlink" href="#raw_block_volumes" title="Permanent link">&para;</a></h3>
<p>The default <code>volumeMode</code> for a <code>PersistentVolumeClaim</code> is <code>Filesystem</code>. If a raw block volume is desired, <code>volumeMode</code> needs to be set to <code>Block</code>. No filesystem will be created.</p>
<div class="admonition tip">
<p class="admonition-title">Good to know</p>
<p>Raw block volumes are required for KubeVirt and OpenShift Virtualization to create OS images and instantiate virtual machines.</p>
</div>
<p>Example raw block volume <code>PVC</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-block
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
  storageClassName: hpe-scod
  volumeMode: Block
</code></pre></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>accessModes</code> may be set to <code>ReadWriteOnce</code>, <code>ReadWriteMany</code> or <code>ReadOnlyMany</code>. It's expected that the application handles read/write IO, volume locking and access in the event of concurrent block access from multiple nodes. Consult the <a href="container_storage_provider/hpe_alletra_6000/index.html#limitations">Alletra 6000 CSP documentation</a> if using <code>ReadWriteMany</code> raw block volumes with FC on Nimble, Alletra 5000 or 6000.</p>
</div>
<p>Mapping the device in a <code>Pod</code> specification is slightly different than using regular filesystems as a <code>volumeDevices</code> section is added instead of a <code>volumeMounts</code> stanza:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Pod
metadata:
  name: my-pod-block
spec:
  containers:
    - name: my-null-pod
      image: fedora:31
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
      args: [ &quot;tail -f /dev/null&quot; ]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: my-pvc-block
</code></pre></p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>There's an in-depth tutorial available on HPE Developer that covers raw block volumes: <a href="https://developer.hpe.com/blog/EE2QnZBXXwi4o7X0E4M0/using-raw-block-and-ephemeral-inline-volumes-on-kubernetes">Using Raw Block Volumes on Kubernetes</a></p>
</div>
<h3 id="using_csi_snapshots">Using CSI Snapshots<a class="headerlink" href="#using_csi_snapshots" title="Permanent link">&para;</a></h3>
<p>CSI introduces snapshots as native objects in Kubernetes that allows end-users to provision <code>VolumeSnapshot</code> objects from an existing <code>PersistentVolumeClaim</code>. New PVCs may then be created using the snapshot as a source.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Ensure <a href="#enabling_csi_snapshots">CSI snapshots are enabled</a>.
<br />There's a <a href="../learn/video_gallery/index.html#using_the_hpe_csi_driver_to_create_csi_snapshots_and_clones">tutorial in the Video Gallery</a> on how to use CSI snapshots and clones.</p>
</div>
<p>Start by creating a <code>VolumeSnapshotClass</code> referencing the <code>Secret</code> and defining additional snapshot parameters.</p>
<p> <pre><code class=yaml>apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: hpe-snapshot
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot;
driver: csi.hpe.com
deletionPolicy: Delete
parameters:
  description: &quot;Snapshot created by the HPE CSI Driver&quot;
  csi.storage.k8s.io/snapshotter-secret-name: hpe-backend
  csi.storage.k8s.io/snapshotter-secret-namespace: hpe-storage
  csi.storage.k8s.io/snapshotter-list-secret-name: hpe-backend
  csi.storage.k8s.io/snapshotter-list-secret-namespace: hpe-storage
</code></pre></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a href="container_storage_provider/index.html">Container Storage Providers</a> may have optional parameters to the <code>VolumeSnapshotClass</code>.</p>
</div>
<p>Create a <code>VolumeSnapshot</code>. This will create a new snapshot of the volume.</p>
<p> <pre><code class=yaml>apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: my-snapshot
spec:
  source:
    persistentVolumeClaimName: my-pvc
</code></pre></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If a specific <code>VolumeSnapshotClass</code> is desired, use <code>.spec.volumeSnapshotClassName</code> to call it out.</p>
</div>
<p>Check that a new <code>VolumeSnapshot</code> is created based on your claim:</p>
<p> <pre><code class=text>kubectl describe volumesnapshot my-snapshot
Name:         my-snapshot
Namespace:    default
...
Status:
  Creation Time:  2019-05-22T15:51:28Z
  Ready:          true
  Restore Size:   32Gi
</code></pre></p>
<p>It's now possible to create a new <code>PersistentVolumeClaim</code> from the <code>VolumeSnapshot</code>.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-snapshot
spec:
  dataSource:
    name: my-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
</code></pre></p>
<div class="admonition caution">
<p class="admonition-title">Important</p>
<p>In HPE CSI Driver v2.5.2 and earlier, The size in <code>.spec.resources.requests.storage</code> must match the <code>.spec.dataSource</code> size.</p>
</div>
<p>The <code>.data.dataSource</code> attribute may also clone <code>PersistentVolumeClaim</code> directly, without creating a <code>VolumeSnapshot</code>.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-pvc
spec:
  dataSource:
    name: my-pvc
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
</code></pre></p>
<p>Again, in HPE CSI Driver v2.5.2 and earlier, the size in <code>.spec.resources.requests.storage</code> must match the source <code>PersistentVolumeClaim</code>. This can get sticky from an automation perspective as volume expansion is being used on the source volume. It's recommended to inspect the source <code>PersistentVolumeClaim</code> or <code>VolumeSnapshot</code> size prior to creating a clone.</p>
<div class="admonition seealso">
<p class="admonition-title">Learn more</p>
<p>For a more comprehensive tutorial on how to use snapshots and clones with CSI on Kubernetes 1.17, see <a href="https://developer.hpe.com/blog/PklOy39w8NtX6M2RvAxW/hpe-csi-driver-for-kubernetes-snapshots-clones-and-volume-expansion">HPE CSI Driver for Kubernetes: Snapshots, Clones and Volume Expansion</a> on HPE Developer.</p>
</div>
<h3 id="volume_groups">Volume Groups<a class="headerlink" href="#volume_groups" title="Permanent link">&para;</a></h3>
<p><code>PersistentVolumeClaims</code> created in a particular <code>Namespace</code> from the same storage backend may be grouped together in a <code>VolumeGroup</code>. A <code>VolumeGroup</code> is what may be known as a "consistency group" in other storage infrastructure systems. This allows certain attributes to be managed on a abstract group and attributes then applies to all member volumes in the group instead of managing each volume individually. One such aspect is creating snapshots with referential integrity between volumes or setting a performance attribute that would have accounting made on the logical group rather than the individual volume.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A tutorial on how to use <code>VolumeGroups</code> and <code>SnapshotGroups</code> is available in the <a href="../learn/video_gallery/index.html#synchronize_volume_snapshots_for_distributed_workloads">Video Gallery</a>.</p>
</div>
<p>Before grouping <code>PeristentVolumeClaims</code> there needs to be a <code>VolumeGroupClass</code> created. It needs to reference a <code>Secret</code> that corresponds to the same backend the <code>PersistentVolumeClaims</code> were created on. A <code>VolumeGroupClass</code> is a cluster resource that needs administrative privileges to create.</p>
<p> <pre><code class=yaml>apiVersion: storage.hpe.com/v1
kind: VolumeGroupClass
metadata:
  name: my-volume-group-class
provisioner: csi.hpe.com
deletionPolicy: Delete
parameters:
  description: &quot;HPE CSI Driver for Kubernetes Volume Group&quot;
  csi.hpe.com/volume-group-provisioner-secret-name: hpe-backend
  csi.hpe.com/volume-group-provisioner-secret-namespace: hpe-storage
</code></pre></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>VolumeGroupClass</code> <code>.parameters</code> may contain CSP specifc parameters. Check the documentation of the <a href="container_storage_provider/index.html">Container Storage Provider</a> being used.</p>
</div>
<p>Once the <code>VolumeGroupClass</code> is in place, users may create <code>VolumeGroups</code>. The <code>VolumeGroups</code> are just like <code>PersistentVolumeClaims</code> part of a <code>Namespace</code> and both resources need to be in the same <code>Namespace</code> for the grouping to be successful.</p>
<p> <pre><code class=yaml>apiVersion: storage.hpe.com/v1
kind: VolumeGroup
metadata:
  name: my-volume-group
spec:
  volumeGroupClassName: my-volume-group-class
</code></pre></p>
<p>Depending on the CSP being used, the <code>VolumeGroup</code> may reference an object that corresponds to the Kubernetes API object. It's not until users annotates their <code>PersistentVolumeClaims</code> the <code>VolumeGroup</code> gets populated.</p>
<p>Adding a <code>PersistentVolumeClaim</code> to a <code>VolumeGroup</code>:</p>
<p> <pre><code class=text>kubectl annotate pvc/my-pvc csi.hpe.com/volume-group=my-volume-group
</code></pre></p>
<p>Removing a <code>PersistentVolumeClaim</code> from a <code>VolumeGroup</code>:</p>
<p> <pre><code class=text>kubectl annotate pvc/my-pvc csi.hpe.com/volume-group-
</code></pre></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>While adding the <code>PersistentVolumeClaim</code> to the <code>VolumeGroup</code> is instant, removal require one reconciliation loop and might not immediately be reflected on the <code>VolumeGroup</code> object.</p>
</div>
<h3 id="snapshot_groups">Snapshot Groups<a class="headerlink" href="#snapshot_groups" title="Permanent link">&para;</a></h3>
<p>Being able to create snapshots of the <code>VolumeGroup</code> require the CSI external-snapshotter to be <a href="#enabling_csi_snapshots">installed</a> and also require a <code>VolumeSnapshotClass</code> <a href="#using_csi_snapshots">configured</a> using the same storage backend as the <code>VolumeGroup</code>. Once those pieces are in place, a <code>SnapshotGroupClass</code> needs to be created. <code>SnapshotGroupClasses</code> are cluster objects created by an administrator.</p>
<p> <pre><code class=yaml>apiVersion: storage.hpe.com/v1
kind: SnapshotGroupClass
metadata:
  name: my-snapshot-group-class
snapshotter: csi.hpe.com
deletionPolicy: Delete
parameters:
  csi.hpe.com/snapshot-group-snapshotter-secret-name: hpe-backend
  csi.hpe.com/snapshot-group-snapshotter-secret-namespace: hpe-storage
</code></pre></p>
<p>Creating a <code>SnapshotGroup</code> is later performed using the <code>VolumeGroup</code> as a source while referencing a <code>SnapshotGroupClass</code> and a <code>VolumeSnapshotClass</code>.</p>
<p> <pre><code class=yaml>apiVersion: storage.hpe.com/v1
kind: SnapshotGroup
metadata:
  name: my-snapshot-group-1
spec:
  source:
    kind: VolumeGroup
    apiGroup: storage.hpe.com
    name: my-volume-group
  snapshotGroupClassName: my-snapshot-group-class
  volumeSnapshotClassName: hpe-snapshot
</code></pre></p>
<p>Once the <code>SnapshotGroup</code> has been successfully created, the individual <code>VolumeSnapshots</code> are now available in the <code>Namespace</code>.</p>
<p>List <code>VolumeSnapshots</code>:</p>
<p> <pre><code class=text>kubectl get volumesnapshots
</code></pre></p>
<p>If no <code>VolumeSnapshots</code> are being enumerated, check the <a href="diagnostics.html#volume_and_snapshot_groups">diagnostics</a> on how to check the component logs and such.</p>
<div class="admonition tip">
<p class="admonition-title">New feature!</p>
<p>Volume Groups and Snapshot Groups got introduced in HPE CSI Driver for Kubernetes 1.4.0.</p>
</div>
<h3 id="expanding_pvcs">Expanding PVCs<a class="headerlink" href="#expanding_pvcs" title="Permanent link">&para;</a></h3>
<p>To perform expansion operations on Kubernetes 1.14+, you must enhance your <code>StorageClass</code> with the <code>.allowVolumeExpansion: true</code> key. Please see <a href="#base_storageclass_parameters">base <code>StorageClass</code> parameters</a> for additional information.</p>
<p>Then, a volume provisioned by a <code>StorageClass</code> with expansion attributes may have its <code>PersistentVolumeClaim</code> expanded by altering the <code>.spec.resources.requests.storage</code> key of the <code>PersistentVolumeClaim</code>.</p>
<p>This may be done by the <code>kubectl patch</code> command.</p>
<p> <pre><code class=text>kubectl patch pvc/my-pvc --patch '{&quot;spec&quot;: {&quot;resources&quot;: {&quot;requests&quot;: {&quot;storage&quot;: &quot;64Gi&quot;}}}}'
persistentvolumeclaim/my-pvc patched
</code></pre></p>
<p>The new <code>PersistentVolumeClaim</code> size may be observed with <code>kubectl get pvc/my-pvc</code> after a few moments.</p>
<h3 id="using_pvc_overrides">Using PVC Overrides<a class="headerlink" href="#using_pvc_overrides" title="Permanent link">&para;</a></h3>
<p>The HPE CSI Driver allows the <code>PersistentVolumeClaim</code> to override the <code>StorageClass</code> parameters by annotating the <code>PersistentVolumeClaim</code>. Define the parameters allowed to be overridden in the <code>StorageClass</code> by setting the <code>allowOverrides</code> parameter:</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-scod-override
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  description: &quot;Volume provisioned by the HPE CSI Driver&quot;
  accessProtocol: iscsi
  allowOverrides: description,accessProtocol
</code></pre></p>
<p>The end-user may now control those parameters (the <code>StorageClass</code> provides the default values).</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-override
  annotations:
    csi.hpe.com/description: &quot;This is my custom description&quot;
    csi.hpe.com/accessProtocol: fc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: hpe-scod-override
</code></pre></p>
<h3 id="using_volume_mutations">Using Volume Mutations<a class="headerlink" href="#using_volume_mutations" title="Permanent link">&para;</a></h3>
<p>The HPE CSI Driver (version 1.3.0 and later) allows the CSP backend volume to be mutated by annotating the <code>PersistentVolumeClaim</code>. Define the parameters allowed to be mutated in the <code>StorageClass</code> by setting the <code>allowMutations</code> parameter.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>There's a tutorial available on YouTube accessible through the <a href="../learn/video_gallery/index.html#adapt_stateful_workloads_dynamically_with_the_hpe_csi_driver_for_kubernetes">Video Gallery</a> on how to use volume mutations to adapt stateful workloads with the HPE CSI Driver.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Important</p>
<p>In order to mutate a <code>StorageClass</code> parameter it needs to have a default value set in the <code>StorageClass</code>. In the example below we'll allow mutatating "description". If the parameter "description" wasn't set when the <code>PersistentVolume</code> was provisioned, no subsequent mutations are allowed. The CSP may set defaults for certain parameters during provisioning, if those are mutable, the mutation will be performed.</p>
</div>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-scod-mutation
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  description: &quot;Volume provisioned by the HPE CSI Driver&quot;
  allowMutations: description
</code></pre></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>allowMutations</code> parameter is a comma separated list of values defined by each of the CSPs parameters, except the <code>description</code> parameter, which is common across all CSPs. See the documentation for each <a href="container_storage_provider/index.html">CSP</a> on what parameters are mutable.</p>
</div>
<p>The end-user may now control those parameters by editing or patching the <code>PersistentVolumeClaim</code>.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-mutation
  annotations:
    csi.hpe.com/description: &quot;My description needs to change&quot;
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: hpe-scod-mutation
</code></pre></p>
<div class="admonition tip">
<p class="admonition-title">Good to know</p>
<p>As the <code>.spec.csi.volumeAttributes</code> on the <code>PersistentVolume</code> are immutable, the mutations performed on the backend volume are also annotated on the <code>PersistentVolume</code> object.</p>
</div>
<h3 id="using_the_nfs_server_provisioner">Using the NFS Server Provisioner<a class="headerlink" href="#using_the_nfs_server_provisioner" title="Permanent link">&para;</a></h3>
<p>The NFS Server Provisioner is required for CSPs that only supports block. If HPE Alletra Storage MP B10000 is being used, consider using the <a href="container_storage_provider/hpe_alletra_storage_mp_b10000_file_service/index.html">File Service CSP</a> instead.</p>
<p>Enabling the NFS Server Provisioner to allow "ReadWriteMany" and "ReadOnlyMany" access mode for a <code>PVC</code> is straightforward. Create a new <code>StorageClass</code> and set <code>.parameters.nfsResources</code> to <code>"true"</code>. Any subsequent claim to the <code>StorageClass</code> will create a NFS server <code>Deployment</code> on the cluster with the associated objects running on top of a "ReadWriteOnce" <code>PVC</code>.</p>
<p>Any "RWO" claim made against the <code>StorageClass</code> will also create a NFS server <code>Deployment</code>. This allows diverse connectivity options among the Kubernetes worker nodes as the HPE CSI Driver will look for nodes labelled <code>csi.hpe.com/hpe-nfs=true</code> (or using a custom value specified in <code>.parameters.nfsNodeSelector</code>) before submitting the workload for scheduling. This allows dedicated NFS worker nodes without user workloads using taints and tolerations. The NFS server <code>Pod</code> is armed with a <code>csi.hpe.com/hpe-nfs</code> toleration. It's required to taint dedicated NFS worker nodes if they truly need to be dedicated.</p>
<p>By default, the NFS Server Provisioner deploy resources in the "hpe-nfs" <code>Namespace</code>. This makes it easy to manage and diagnose. However, to use CSI data management capabilities (<code>VolumeSnapshots</code> and <code>.spec.dataSource</code>) on the PVCs, the NFS resources need to be deployed in the same <code>Namespace</code> as the "RWX"/"ROX" requesting <code>PVC</code>. This is controlled by the <code>nfsNamespace</code> <code>StorageClass</code> parameter. See <a href="#base_storageclass_parameters">base <code>StorageClass</code> parameters</a> for more information.</p>
<p>Under no circumstance should the NFS Server Provisioned <code>PVCs</code> be used to create KubeVirt images or boot VMs from. Use <a href="#raw_block_volumes">Raw Block Volumes</a> for KubeVirt and OpenShift Virtualization.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A comprehensive <a href="https://developer.hpe.com/blog/xABwJY56qEfNGMEo1lDj/introducing-a-nfs-server-provisioner-and-pod-monitor-for-the-hpe-csi-dri">tutorial is available</a> on HPE Developer on how to get started with the NFS Server Provisioner and the HPE CSI Driver for Kubernetes.</p>
</div>
<p>Example <code>StorageClass</code> with "nfsResources" enabled. No CSP specific parameters for clarity.</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-standard-file
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: &quot;NFS volume created by the HPE CSI Driver for Kubernetes&quot;
  csi.storage.k8s.io/fstype: ext4
  nfsResources: &quot;true&quot;
reclaimPolicy: Delete
allowVolumeExpansion: true
</code></pre></p>
<p>Example use of <code>accessModes</code>:</p>
<div class=md-fenced-code-tabs id=tab-tab-group-35><input name=tab-group-35 type=radio id=tab-group-35-0_yaml checked=checked class=code-tab data-lang=yaml aria-controls=tab-group-35-0_yaml-panel role=tab><label for=tab-group-35-0_yaml class=code-tab-label data-lang=yaml id=tab-group-35-0_yaml-label>ReadWriteOnce</label><div class=code-tabpanel role=tabpanel data-lang=yaml id=tab-group-35-0_yaml-panel aria-labelledby=tab-group-35-0_yaml-label><pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-rwo-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
  storageClassName: hpe-nfs
</code></pre></div><input name=tab-group-35 type=radio id=tab-group-35-1_yaml class=code-tab data-lang=yaml aria-controls=tab-group-35-1_yaml-panel role=tab><label for=tab-group-35-1_yaml class=code-tab-label data-lang=yaml id=tab-group-35-1_yaml-label>ReadWriteMany</label><div class=code-tabpanel role=tabpanel data-lang=yaml id=tab-group-35-1_yaml-panel aria-labelledby=tab-group-35-1_yaml-label><pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-rwx-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 32Gi
  storageClassName: hpe-nfs
</code></pre></div><input name=tab-group-35 type=radio id=tab-group-35-2_yaml class=code-tab data-lang=yaml aria-controls=tab-group-35-2_yaml-panel role=tab><label for=tab-group-35-2_yaml class=code-tab-label data-lang=yaml id=tab-group-35-2_yaml-label>ReadOnlyMany</label><div class=code-tabpanel role=tabpanel data-lang=yaml id=tab-group-35-2_yaml-panel aria-labelledby=tab-group-35-2_yaml-label><pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-rox-pvc
spec:
  accessModes:
  - ReadOnlyMany
  resources:
    requests:
      storage: 32Gi
  storageClassName: hpe-nfs
</code></pre></div></div>
<p>In the case of declaring a "ROX" <code>PVC</code>, the requesting <code>Pod</code> specification needs to request the <code>PVC</code> as read-only. Example:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Pod
metadata:
  name: pod-rox
spec:
  containers:
  - image: busybox
    name: busybox
    command:
      - &quot;sleep&quot;
      - &quot;300&quot;
    volumeMounts:
    - mountPath: /data
      name: my-vol
      readOnly: true
  volumes:
  - name: my-vol
    persistentVolumeClaim:
      claimName: my-rox-pvc
      readOnly: true
</code></pre></p>
<p>Requesting an empty read-only volume might not seem practical. The primary use case is to source existing datasets into immutable applications, using either a backend CSP cloning capability or CSI data management feature such as <a href="#using_csi_snapshots">snapshots or existing PVCs</a>.</p>
<h4 id="using_a_foreign_storageclass">Using a Foreign StorageClass<a class="headerlink" href="#using_a_foreign_storageclass" title="Permanent link">&para;</a></h4>
<p>Since HPE CSI Driver for Kubernetes version 2.4.1 it's possible to provision NFS servers on top of non-HPE CSI Driver <code>StorageClasses</code>. The most prominent use case for this functionality is to coexist with the vSphere CSI Driver (VMware vSphere Container Storage Plug-in) in FC environments and provide "RWX" <code>PVCs</code>.</p>
<h5 id="example_storageclass_using_a_foreign_storageclass">Example StorageClass using a foreign StorageClass<a class="headerlink" href="#example_storageclass_using_a_foreign_storageclass" title="Permanent link">&para;</a></h5>
<p>The HPE CSI Driver only manages the NFS server <code>Deployment</code>, <code>Service</code> and <code>PVC</code>. There must be an existing <code>StorageClass</code> capable of provisioning "RWO" filesystem <code>PVCs</code>.</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-nfs-servers
provisioner: csi.hpe.com
parameters:
  nfsResources: &quot;true&quot;
  nfsForeignStorageClass: &quot;my-foreign-storageclass-name&quot;
reclaimPolicy: Delete
allowVolumeExpansion: false
</code></pre></p>
<p>Next, provision "RWO" or "RWX" claims from the "hpe-nfs-servers" <code>StorageClass</code>. An NFS server will be provisioned on a "RWO" <code>PVC</code> from the <code>StorageClass</code> "my-foreign-storageclass-name".</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only <code>StorageClasses</code> that uses HPE storage proxied by partner CSI drivers are supported by HPE.</p>
</div>
<h4 id="limitations_and_considerations_for_the_nfs_server_provisioner">Limitations and Considerations for the NFS Server Provisioner<a class="headerlink" href="#limitations_and_considerations_for_the_nfs_server_provisioner" title="Permanent link">&para;</a></h4>
<p>These are some common issues and gotchas that are useful to know about when planning to use the NFS Server Provisioner.</p>
<ul>
<li>The current tested and supported limit for the NFS Server Provisioner is 32 NFS servers per Kubernetes worker node.</li>
<li>The two <code>StorageClass</code> parameters "nfsResourceLimitsCpuM" and "nfsResourceLimitsMemoryMi" control how much CPU and memory it may consume. Tests show that the NFS server consumes about 150MiB at instantiation and 2GiB is the recommended minimum for most workloads. The NFS server <code>Pod</code> is by default limited to 2GiB of memory and 1000 milli CPU.</li>
<li>HPE CSI Driver v2.5.2 or later is required to expand NFS <code>PersistentVolumeClaims</code>.</li>
<li>Due to the fact that the NFS Server Provisioner deploys a number of different resources on the hosting cluster per <code>PVC</code>, provisioning times may differ greatly between clusters. On an idle cluster with the NFS Server Provisioning image cached, less than 30 seconds is the most common sighting but it may exceed 30 seconds which may trigger warnings on the requesting <code>PVC</code>. This is normal behavior.</li>
<li>The HPE CSI Driver includes a Pod Monitor to delete <code>Pods</code> that have become unavailable due to the Pod status changing to <code>NodeLost</code> or a node becoming unreachable that the <code>Pod</code> runs on. By default the Pod Monitor only watches the NFS Server Provisioner <code>Deployments</code>. It may be used for any <code>Deployment</code>. See <a href="monitor.html">Pod Monitor</a> on how to use it, especially the <a href="monitor.html#limitations">limitations</a>.</li>
<li>Certain CNIs may have issues to gracefully restore access from the NFS clients to the NFS export. Flannel have exhibited this problem and the most consistent performance have been observed with Calico.</li>
<li>The <a href="#using_volume_mutations">Volume Mutation</a> feature does not work on the NFS <code>PVC</code>. If changes are needed, perform the change on the backing "ReadWriteOnce" <code>PVC</code>.</li>
<li>As outlined in <a href="#using_the_nfs_server_provisioner">Using the NFS Server Provisioner</a>, CSI snapshots and cloning of NFS <code>PVCs</code> requires the CSI snapshot and NFS server to reside in the same <code>Namespace</code>. This also applies when using third-party backup software such as Veeam Kasten. Use the "nfsNamespace" <code>StorageClass</code> parameter to control where to provision resources.</li>
<li><a href="using.html#volume_groups">VolumeGroups</a> and <a href="using.html#snapshot_groups">SnapshotGroups</a> are only supported on the backing "ReadWriteOnce" <code>PVC</code>. The "volume-group" annotation may be set at the initial creation of the NFS <code>PVC</code> but will have adverse effect on logging as the Volume Group Provisioner tries to add the NFS <code>PVC</code> to the backend consistency group indefinitely.</li>
<li>The NFS servers deployed by the HPE CSI Driver are not managed during CSI driver upgrades. Manual <a href="operations.html#upgrade_nfs_servers">upgrade is required</a>.</li>
<li>Using the same network interface for NFS and block IO has shown suboptimal performance. Use FC for the block storage for the best performance.</li>
<li>A single NFS server instance is capable of 100GigE wirespeed with large sequential workloads and up to 200,000 IOPS with small IO using bare-metal nodes and multiple clients.</li>
<li>Using ext4 as the backing filesystem has shown better performance with simultaneous writers to the same file.</li>
<li>Additional configuration and considerations may be required when using the NFS Server Provisioner with Red Hat OpenShift. See <a href="partners/redhat_openshift/index.html#nfs_server_provisioner_considerations">NFS Server Provisioner Considerations</a> for OpenShift.</li>
<li>XFS has proven troublesome to use as a backend "RWO" volume filesystem prior to HPE CSI Driver v3.0.0, leaving stale NFS handles for clients. Use ext4 as the "csi.storage.k8s.io/fstype" <code>StorageClass</code> parameter for best results.</li>
<li>The NFS servers provide a "ClusterIP" <code>Service</code>. It is possible to expose the NFS servers outside the cluster for external NFS clients. Understand the scope and limitations in <a href="operations.html#expose_nfs_services_outside_of_the_kubernetes_cluster">Auxillary Operations</a>.</li>
<li>If the NFS Server Provisioner is unable to bring up the <code>Deployment</code> for some reason, the operation is rolled back and restarted, this may cause unnecessary churn and always make sure there are plenty of resources available where the NFS servers are scheduled.</li>
<li>The liveness probe that will restart the NFS Deployment if the exported filesystem becomes read-only will get stuck if there's an all paths down situation. Provide redundant paths to avoid this issue and if it does get stuck, repair at least one data path.</li>
<li>The NFS <code>Pod</code> "fsGroup" is mapped to "nobody". This may be changed in the running NFS <code>Deployments</code> by following <a href="operations.html#change_default_fsgroup_for_nfs_servers">these procedures</a> to match a frontend workload "fsGroup".</li>
</ul>
<p>See <a href="diagnostics.html#nfs_server_provisioner_resources">diagnosing NFS Server Provisioner issues</a> for further details.</p>
<h3 id="using_volume_encryption">Using Volume Encryption<a class="headerlink" href="#using_volume_encryption" title="Permanent link">&para;</a></h3>
<p>From version 2.0.0 and onwards of the CSI driver supports host-based volume encryption for any of the CSPs supported by the CSI driver.</p>
<p>Host-based volume encryption is controlled by <code>StorageClass</code> parameters configured by the Kubernetes administrator and may be configured to be overridden by Kubernetes users. In the below example, a single <code>Secret</code> is used to encrypt and decrypt all volumes provisioned by the <code>StorageClass</code>.</p>
<p>First, create a <code>Secret</code>, in this example we'll use the "hpe-storage" <code>Namespace</code>.</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: Secret
metadata:
  name: my-passphrase
  namespace: hpe-storage
stringData:
  hostEncryptionPassphrase: &quot;HPE CSI Driver for Kubernetes 2.0.0 Rocks!&quot;
</code></pre></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The "hostEncryptionPassphrase" can be up to 512 characters.</p>
</div>
<p>Next, incorporate the <code>Secret</code> into a <code>StorageClass</code>.</p>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hpe-encrypted
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  description: &quot;Volume provisioned by the HPE CSI Driver&quot;
  hostEncryption: &quot;true&quot;
  hostEncryptionSecretName: my-passphrase
  hostEncryptionSecretNamespace: hpe-storage
reclaimPolicy: Delete
allowVolumeExpansion: true
</code></pre></p>
<p>Next, create a <code>PersistentVolumeClaim</code> that uses the "hpe-encrypted" <code>StorageClass</code>:</p>
<p> <pre><code class=yaml>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-encrypted-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: hpe-encrypted
</code></pre></p>
<p>Attach a basic <code>Pod</code> to verify functionality.</p>
<p> <pre><code class=yaml>kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: pod-datelog-1
      image: nginx
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
    - name: pod-datelog-2
      image: debian
      command: [&quot;bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do date &gt;&gt; /data/mydata.txt; sleep 1; done&quot;]
      volumeMounts:
        - name: export1
          mountPath: /data
  volumes:
    - name: export1
      persistentVolumeClaim:
        claimName: my-encrypted-pvc
</code></pre></p>
<p>Once the <code>Pod</code> comes up, verify that the volume is encrypted.</p>
<p> <pre><code class=text>$ kubectl exec -it my-pod -c pod-datelog-1 -- df -h /data
Filesystem              Size  Used Avail Use% Mounted on
/dev/mapper/enc-mpatha  100G   33M  100G   1% /data
</code></pre></p>
<p>Host-based volume encryption is in effect if the "enc" prefix is seen on the multipath device name.</p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>For an in-depth tutorial and more advanced use cases for host-based volume encryption, check out this blog post on HPE Developer: <a href="https://developer.hpe.com/blog/host-based-volume-encryption-with-hpe-csi-driver-for-kubernetes/">Host-based Volume Encryption with HPE CSI Driver for Kubernetes</a></p>
</div>
<h3 id="topology_and_volumebindingmode">Topology and volumeBindingMode<a class="headerlink" href="#topology_and_volumebindingmode" title="Permanent link">&para;</a></h3>
<p>With CSI driver v2.5.0 and newer, basic CSI topology information can be associated with a single backend from a <code>StorageClass</code>. For backwards compatibility, only <code>volumeBindingMode: WaitForFirstConsumer</code> require topology labels assigned to compute nodes. Using the default <code>volumeBindingMode</code> of <code>Immediate</code> will preserve the behavior prior to v2.5.0.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The "csi-provisioner" is deployed with <code>--feature-gates Topology=true</code> and <code>--immediate-topology=false</code>. It's impact on volume provisioning and accessibility can be found <a href="https://github.com/kubernetes-csi/external-provisioner?tab=readme-ov-file#topology-support">here</a>.</p>
</div>
<p>Assume a simple use case where only a handful of nodes in a Kubernetes cluster have Fibre Channel adapters installed. Workloads with persistent storage requirements from a particular <code>StorageClass</code> should be deployed onto those nodes only.</p>
<h4 id="label_compute_nodes">Label Compute Nodes<a class="headerlink" href="#label_compute_nodes" title="Permanent link">&para;</a></h4>
<p>Nodes with the label <code>csi.hpe.com/zone</code> are considered during topology accessibility assessments. Assume three nodes in the cluster have FC adapters.</p>
<p> <pre><code class=text>kubectl label node/my-node{1..3} csi.hpe.com/zone=fc --overwrite
</code></pre></p>
<p>If the CSI driver is already installed on the cluster, the CSI node driver needs to be restarted for the node labels to propagate.</p>
<p> <pre><code class=text>kubectl rollout restart -n hpe-storage ds/hpe-csi-node
</code></pre></p>
<h4 id="create_storageclass_with_topology_information">Create StorageClass with Topology Information<a class="headerlink" href="#create_storageclass_with_topology_information" title="Permanent link">&para;</a></h4>
<p> <pre><code class=yaml>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: hpe-standard-fc
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: hpe-backend
  csi.storage.k8s.io/controller-expand-secret-namespace: hpe-storage
  csi.storage.k8s.io/controller-publish-secret-name: hpe-backend
  csi.storage.k8s.io/controller-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-publish-secret-name: hpe-backend
  csi.storage.k8s.io/node-publish-secret-namespace: hpe-storage
  csi.storage.k8s.io/node-stage-secret-name: hpe-backend
  csi.storage.k8s.io/node-stage-secret-namespace: hpe-storage
  csi.storage.k8s.io/provisioner-secret-name: hpe-backend
  csi.storage.k8s.io/provisioner-secret-namespace: hpe-storage
  description: &quot;Volume created by the HPE CSI Driver for Kubernetes&quot;
  accessProtocol: fc
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: csi.hpe.com/zone
    values:
    - fc
</code></pre></p>
<p>Any workload provisioning <code>PVCs</code> from the above <code>StorageClass</code> will now be scheduled on nodes labeled <code>csi.hpe.com/zone=fc</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>allowedTopologies</code> key may be omitted if there's only a single topology applied to a subset of nodes. The nodes always need to be labeled when using <code>volumeBindingMode: WaitForFirstConsumer</code>. If all nodes have access to a backend, set <code>volumeBindingMode: Immediate</code> and omit <code>allowedTopologies</code>.</p>
</div>
<h3 id="static_provisioning">Static Provisioning<a class="headerlink" href="#static_provisioning" title="Permanent link">&para;</a></h3>
<p>How to map an existing backend volume to a <code>PersistentVolume</code> differs between the CSP implementations.</p>
<ul>
<li><a href="container_storage_provider/hpe_alletra_6000/index.html#static_provisioning">HPE Alletra 5000/6000 and Nimble Storage</a></li>
<li><a href="container_storage_provider/hpe_alletra_storage_mp_b10000/index.html#static_provisioning">HPE Alletra Storage MP B10000, Alletra 9000, Primera and 3PAR</a></li>
</ul>
<h2 id="further_reading">Further Reading<a class="headerlink" href="#further_reading" title="Permanent link">&para;</a></h2>
<p>The <a href="https://kubernetes.io/docs/concepts/storage/volumes/">official Kubernetes documentation</a> contains comprehensive documentation on how to markup <code>PersistentVolumeClaim</code> and <code>StorageClass</code> API objects to tweak certain behaviors.</p>
<p>Each CSP has a set of unique <code>StorageClass</code> parameters that may be tweaked to accommodate a wide variety of use cases. Please see the documentation of <a href="container_storage_provider/index.html">the respective CSP for more details</a>.</p>
              
            </div>
          </div>

<footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2020-2025 Hewlett Packard Enterprise Development LP<br />Give <a href="https://github.com/hpe-storage/scod/issues/new?title=csi_driver/using.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/hpe-storage/scod" class="fa fa-code-fork" style="color: #fcfcfc"> hpe-storage/scod</a>
        </span>
    
    
      <span><a href="deployment.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="container_storage_provider/hpe_alletra_storage_mp_b10000/index.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
